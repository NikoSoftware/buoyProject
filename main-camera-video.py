import cv2
import numpy as np
import time
import os
from datetime import datetime
from ais_bench.infer.interface import InferSession


class NPUInferencer:
    def __init__(self, model_path, conf_threshold=0.3):
        self.model = InferSession(device_id=0, model_path=model_path)
        self.conf_threshold = conf_threshold
        self.input_shape = self.model.get_inputs()[0].shape
        self.output_shape = self.model.get_outputs()[0].shape
        self.total_inference_time = 0
        self.frame_count = 0
        # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•
        self.video_dir = ".video"
        os.makedirs(self.video_dir, exist_ok=True)

        # ç±»åˆ«æ˜ å°„è¡¨
        self.class_names = {
            0: "buoy",  # å‡è®¾çº¢è‰²å°ç“¶å­æ˜¯ç±»åˆ«0
            # æ·»åŠ æ›´å¤šç±»åˆ«å¦‚: 1: "person", 2: "car"...
        }

        # è§†é¢‘åˆ†æ®µå‚æ•°
        self.segment_interval = 20  # æ¯20ç§’ä¿å­˜ä¸€ä¸ªè§†é¢‘ç‰‡æ®µ
        self.segment_start_time = time.time()
        self.segment_count = 0
        self.out_video = None
        self.current_video_path = ""

    def preprocess(self, frame):
        start_time = time.perf_counter()
        img = cv2.resize(frame, (self.input_shape[3], self.input_shape[2]))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img.transpose(2, 0, 1)[np.newaxis]
        img = img.astype("float32") / 255.0
        elapsed = (time.perf_counter() - start_time) * 1000
        print(f"é¢„å¤„ç†è€—æ—¶: {elapsed:.2f}ms")
        return img

    def postprocess(self, outputs, frame):
        start_time = time.perf_counter()
        h, w = frame.shape[:2]

        # ç¡®å®šè¾“å‡ºæ ¼å¼
        if outputs[0].shape == (1, 8400, 84):
            detections = outputs[0][0]
        else:
            detections = outputs[0][0].transpose(1, 0)

        if detections.size == 0:
            return frame

        detected_objects = []  # å­˜å‚¨æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯
        object_count = 0  # æ£€æµ‹åˆ°çš„ç‰©ä½“è®¡æ•°å™¨

        for row in detections:
            if len(row) < 6:
                continue
            try:
                x1, y1, x2, y2, conf, cls_id = row[:6]
                if conf < self.conf_threshold:
                    continue

                # è½¬æ¢ä¸ºå›¾åƒåæ ‡
                x1, y1, x2, y2 = int(x1 * w), int(y1 * h), int(x2 * w), int(y2 * h)
                x_c, y_c = (x1 + x2) // 2, (y1 + y2) // 2  # è´¨å¿ƒåæ ‡

                # è·å–ç±»åˆ«åç§°
                class_name = self.class_names.get(int(cls_id), f"class_{int(cls_id)}")

                # æ‰“å°æ£€æµ‹ä¿¡æ¯
                print(f"[Frame {self.frame_count}] æ£€æµ‹åˆ°ç›®æ ‡: "
                      f"{class_name} (ç½®ä¿¡åº¦: {conf:.2%}), "
                      f"è¾¹ç•Œæ¡†: [{x1}, {y1}, {x2}, {y2}], "
                      f"è´¨å¿ƒä½ç½®: ({x_c}, {y_c})")

                # å­˜å‚¨æ£€æµ‹ä¿¡æ¯
                detected_objects.append({
                    "class": class_name,
                    "confidence": conf,
                    "bbox": (x1, y1, x2, y2),
                    "center": (x_c, y_c)
                })
                object_count += 1

                # ç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
                label = f"{class_name} {conf:.2f}"
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.circle(frame, (x_c, y_c), 5, (0, 0, 255), -1)  # ç»˜åˆ¶è´¨å¿ƒ
                cv2.putText(frame, label, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            except (ValueError, IndexError) as e:
                print(f"å¤„ç†æ£€æµ‹ç»“æœæ—¶å‡ºé”™: {e}")
                continue

        # æ‰“å°æœ¬å¸§æ£€æµ‹ç»Ÿè®¡
        if object_count > 0:
            print(f"=== å¸§ {self.frame_count} æ£€æµ‹ç»Ÿè®¡ ===")
            print(f"æ£€æµ‹åˆ°ç›®æ ‡æ€»æ•°: {object_count}")
            print(f"ä¸»è¦ç›®æ ‡: {detected_objects[0]['class']} "
                  f"(ç½®ä¿¡åº¦: {detected_objects[0]['confidence']:.2%})")
            print("=" * 40)

        elapsed = (time.perf_counter() - start_time) * 1000
        print(f"åå¤„ç†è€—æ—¶: {elapsed:.2f}ms")
        return frame

    def init_video_writer(self, width, height, fps):
        """åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨"""
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_video_path = os.path.join(
            self.video_dir,
            f"segment_{self.segment_count}_{timestamp}.avi"
        )

        # è®¾ç½®è§†é¢‘ç¼–ç å™¨ï¼ˆä½¿ç”¨XVIDç¼–ç ï¼Œå…¼å®¹æ€§å¥½ï¼‰[2,3](@ref)
        fourcc = cv2.VideoWriter_fourcc(*'XVID')

        # åˆ›å»ºVideoWriterå¯¹è±¡
        self.out_video = cv2.VideoWriter(
            self.current_video_path,
            fourcc,
            fps,
            (width, height)
        )

        print(f"\nğŸ”´ å¼€å§‹å½•åˆ¶æ–°è§†é¢‘ç‰‡æ®µ: {self.current_video_path}")
        self.segment_start_time = time.time()
        self.segment_count += 1

    def predict_from_camera(self, show=True, camera_index=0):
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        cap = cv2.VideoCapture(camera_index)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        cap.set(cv2.CAP_PROP_FPS, 30)

        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€USBæ‘„åƒå¤´: /dev/video{camera_index}")

        print(f"æˆåŠŸæ‰“å¼€USBæ‘„åƒå¤´: /dev/video{camera_index}")
        print("æŒ‰ 'q' é”®é€€å‡ºå®æ—¶æ£€æµ‹...")

        # è·å–æ‘„åƒå¤´å‚æ•°
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0:
            fps = 30.0  # é»˜è®¤å€¼

        # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªè§†é¢‘å†™å…¥å™¨
        self.init_video_writer(width, height, fps)

        # æ€§èƒ½ç›‘æ§
        inference_times = []
        last_report_time = time.time()
        report_interval = 5  # æ€§èƒ½æŠ¥å‘Šé—´éš”(ç§’)

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("æ— æ³•ä»æ‘„åƒå¤´è·å–å¸§")
                    time.sleep(0.1)
                    continue

                # å®Œæ•´å¤„ç†æµç¨‹è®¡æ—¶
                frame_start = time.perf_counter()

                # é¢„å¤„ç†
                blob = self.preprocess(frame)

                # NPUæ¨ç†æ ¸å¿ƒè®¡æ—¶
                infer_start = time.perf_counter()
                outputs = self.model.infer(blob, mode="static")
                infer_elapsed = (time.perf_counter() - infer_start) * 1000

                # è®°å½•å¹¶æ‰“å°æ¨ç†è€—æ—¶
                inference_times.append(infer_elapsed)
                self.total_inference_time += infer_elapsed
                self.frame_count += 1

                # åå¤„ç†ï¼ˆç›®æ ‡æ£€æµ‹å’Œæ ‡è®°ï¼‰
                result_frame = self.postprocess(outputs, frame)

                # å¸§å¤„ç†æ€»è€—æ—¶
                frame_elapsed = (time.perf_counter() - frame_start) * 1000

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°çš„è§†é¢‘ç‰‡æ®µ
                current_time = time.time()
                if current_time - self.segment_start_time >= self.segment_interval:
                    # å…³é—­å½“å‰è§†é¢‘æ–‡ä»¶
                    if self.out_video is not None:
                        self.out_video.release()
                        print(f"ğŸŸ¢ è§†é¢‘ç‰‡æ®µå·²ä¿å­˜: {self.current_video_path}")

                    # åˆå§‹åŒ–æ–°çš„è§†é¢‘å†™å…¥å™¨
                    self.init_video_writer(width, height, fps)

                # å°†æ ‡è®°åçš„å¸§å†™å…¥å½“å‰è§†é¢‘æ–‡ä»¶
                if self.out_video is not None:
                    self.out_video.write(result_frame)

                # æ˜¾ç¤ºå¤„ç†åçš„å¸§
                if show:
                    cv2.imshow('NPUåŠ é€Ÿ-YOLO', result_frame)

                # å®šæ—¶æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
                if current_time - last_report_time >= report_interval:
                    avg_time = np.mean(inference_times) if inference_times else 0
                    fps = 1000 / avg_time if avg_time > 0 else 0
                    print(f"\nå®æ—¶æ€§èƒ½æŠ¥å‘Š (è¿‡å»{report_interval}ç§’):")
                    print(f"- å¤„ç†å¸§æ•°: {len(inference_times)}")
                    print(f"- å¹³å‡æ¨ç†è€—æ—¶: {avg_time:.2f}ms")
                    print(f"- é¢„ä¼°FPS: {fps:.1f}")
                    print("-" * 40)

                    # é‡ç½®è®¡æ•°å™¨å’Œæ—¶é—´æˆ³
                    inference_times = []
                    last_report_time = current_time

                # æ£€æŸ¥é€€å‡ºé”®
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        finally:
            # é‡Šæ”¾èµ„æº
            cap.release()
            if self.out_video is not None:
                self.out_video.release()
            cv2.destroyAllWindows()

            # æœ€ç»ˆæ€§èƒ½æ€»ç»“æŠ¥å‘Š
            if self.frame_count > 0:
                avg_time = self.total_inference_time / self.frame_count
                fps = 1000 / avg_time if avg_time > 0 else 0
                print("\n" + "=" * 50)
                print("æœ€ç»ˆæ€§èƒ½åˆ†ææŠ¥å‘Š:")
                print(f"- æ€»å¤„ç†å¸§æ•°: {self.frame_count}")
                print(f"- å¹³å‡æ¨ç†è€—æ—¶: {avg_time:.2f}ms")
                print(f"- é¢„ä¼°å¹³å‡FPS: {fps:.1f}")
                print(f"- ä¿å­˜è§†é¢‘ç‰‡æ®µæ•°: {self.segment_count}")
                print(f"- è§†é¢‘ä¿å­˜ä½ç½®: {os.path.abspath(self.video_dir)}")
                print("=" * 50)


if __name__ == '__main__':
    inferencer = NPUInferencer(
        model_path="./runs/train/train6/weights/best.om",
        conf_threshold=0.3
    )
    inferencer.predict_from_camera(
        camera_index=0,
        show=False
    )