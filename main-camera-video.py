import cv2
import numpy as np
import time
import os
from datetime import datetime
from ais_bench.infer.interface import InferSession


class NPUInferencer:
    def __init__(self, model_path, conf_threshold=0.3):
        self.model = InferSession(device_id=0, model_path=model_path)
        self.conf_threshold = conf_threshold
        self.input_shape = self.model.get_inputs()[0].shape
        self.output_shape = self.model.get_outputs()[0].shape
        self.total_inference_time = 0
        self.frame_count = 0
        # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•
        self.video_dir = ".video"
        os.makedirs(self.video_dir, exist_ok=True)

        # ç±»åˆ«æ˜ å°„è¡¨
        self.class_names = {
            0: "buoy",  # å‡è®¾çº¢è‰²å°ç“¶å­æ˜¯ç±»åˆ«0
            # æ·»åŠ æ›´å¤šç±»åˆ«å¦‚: 1: "person", 2: "car"...
        }

        # è§†é¢‘åˆ†æ®µå‚æ•°
        self.segment_interval = 20  # æ¯20ç§’ä¿å­˜ä¸€ä¸ªè§†é¢‘ç‰‡æ®µ
        self.segment_start_time = time.time()
        self.segment_count = 0
        self.out_video = None
        self.current_video_path = ""
        self.actual_fps = 30.0  # é»˜è®¤å¸§ç‡

    def preprocess(self, frame):
        start_time = time.perf_counter()
        img = cv2.resize(frame, (self.input_shape[3], self.input_shape[2]))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img.transpose(2, 0, 1)[np.newaxis]
        img = img.astype("float32") / 255.0
        elapsed = (time.perf_counter() - start_time) * 1000
        print(f"é¢„å¤„ç†è€—æ—¶: {elapsed:.2f}ms")
        return img

    def postprocess(self, outputs, frame):
        start_time = time.perf_counter()
        h, w = frame.shape[:2]

        # ç¡®å®šè¾“å‡ºæ ¼å¼
        if outputs[0].shape == (1, 8400, 84):
            detections = outputs[0][0]
        else:
            detections = outputs[0][0].transpose(1, 0)

        if detections.size == 0:
            return frame

        detected_objects = []  # å­˜å‚¨æ£€æµ‹åˆ°çš„ç‰©ä½“ä¿¡æ¯
        object_count = 0  # æ£€æµ‹åˆ°çš„ç‰©ä½“è®¡æ•°å™¨

        for row in detections:
            if len(row) < 6:
                continue
            try:
                x1, y1, x2, y2, conf, cls_id = row[:6]
                if conf < self.conf_threshold:
                    continue

                # è½¬æ¢ä¸ºå›¾åƒåæ ‡
                x1, y1, x2, y2 = int(x1 * w), int(y1 * h), int(x2 * w), int(y2 * h)
                x_c, y_c = (x1 + x2) // 2, (y1 + y2) // 2  # è´¨å¿ƒåæ ‡

                # è·å–ç±»åˆ«åç§°
                class_name = self.class_names.get(int(cls_id), f"class_{int(cls_id)}")

                # æ‰“å°æ£€æµ‹ä¿¡æ¯
                print(f"[Frame {self.frame_count}] æ£€æµ‹åˆ°ç›®æ ‡: "
                      f"{class_name} (ç½®ä¿¡åº¦: {conf:.2%}), "
                      f"è¾¹ç•Œæ¡†: [{x1}, {y1}, {x2}, {y2}], "
                      f"è´¨å¿ƒä½ç½®: ({x_c}, {y_c})")

                # å­˜å‚¨æ£€æµ‹ä¿¡æ¯
                detected_objects.append({
                    "class": class_name,
                    "confidence": conf,
                    "bbox": (x1, y1, x2, y2),
                    "center": (x_c, y_c)
                })
                object_count += 1

                # ç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
                label = f"{class_name} {conf:.2f}"
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.circle(frame, (x_c, y_c), 5, (0, 0, 255), -1)  # ç»˜åˆ¶è´¨å¿ƒ
                cv2.putText(frame, label, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            except (ValueError, IndexError) as e:
                print(f"å¤„ç†æ£€æµ‹ç»“æœæ—¶å‡ºé”™: {e}")
                continue

        # æ‰“å°æœ¬å¸§æ£€æµ‹ç»Ÿè®¡
        if object_count > 0:
            print(f"=== å¸§ {self.frame_count} æ£€æµ‹ç»Ÿè®¡ ===")
            print(f"æ£€æµ‹åˆ°ç›®æ ‡æ€»æ•°: {object_count}")
            print(f"ä¸»è¦ç›®æ ‡: {detected_objects[0]['class']} "
                  f"(ç½®ä¿¡åº¦: {detected_objects[0]['confidence']:.2%})")
            print("=" * 40)

        elapsed = (time.perf_counter() - start_time) * 1000
        print(f"åå¤„ç†è€—æ—¶: {elapsed:.2f}ms")
        return frame

    def init_video_writer(self, width, height, fps):
        """åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨"""
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_video_path = os.path.join(
            self.video_dir,
            f"segment_{self.segment_count}_{timestamp}.mp4"  # ä¿®æ”¹ä¸ºMP4æ ¼å¼[1,3](@ref)
        )

        # ä½¿ç”¨å…¼å®¹æ€§æ›´å¥½çš„H.264ç¼–ç å™¨[5](@ref)
        # å°è¯•å¤šç§ç¼–ç å™¨ç¡®ä¿å…¼å®¹æ€§
        fourcc_options = [
            cv2.VideoWriter_fourcc(*'avc1'),  # é¦–é€‰H.264ç¼–ç [5](@ref)
            cv2.VideoWriter_fourcc(*'mp4v'),  # å¤‡ç”¨MPEG-4ç¼–ç [1](@ref)
            cv2.VideoWriter_fourcc(*'X264'),  # å¼€æºH.264å®ç°
            cv2.VideoWriter_fourcc(*'MJPG')  # å…¼å®¹æ€§æœ€å¥½çš„ç¼–ç å™¨[2](@ref)
        ]

        # ç¡®ä¿å¸§ç‡åœ¨åˆç†èŒƒå›´å†…(1-60FPS)
        safe_fps = max(1, min(60, fps))
        print(f"å®‰å…¨å¸§ç‡è®¾ç½®: {safe_fps}FPS")

        # å°è¯•ä¸åŒç¼–ç å™¨ç›´åˆ°æˆåŠŸ
        for fourcc in fourcc_options:
            try:
                self.out_video = cv2.VideoWriter(
                    self.current_video_path,
                    fourcc,
                    safe_fps,
                    (width, height)
                )

                # æ£€æŸ¥æ˜¯å¦æˆåŠŸåˆå§‹åŒ–
                if self.out_video.isOpened():
                    print(f"âœ… æˆåŠŸä½¿ç”¨ç¼–ç å™¨: {fourcc} åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨")
                    break
                else:
                    print(f"âš ï¸ ç¼–ç å™¨ {fourcc} åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªé€‰é¡¹")
                    self.out_video = None
            except Exception as e:
                print(f"ç¼–ç å™¨ {fourcc} åˆå§‹åŒ–é”™è¯¯: {str(e)}")
                self.out_video = None

        # å¦‚æœæ‰€æœ‰ç¼–ç å™¨éƒ½å¤±è´¥
        if self.out_video is None or not self.out_video.isOpened():
            print(f"âŒ é”™è¯¯: æ— æ³•åˆå§‹åŒ–ä»»ä½•è§†é¢‘ç¼–ç å™¨ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿæ”¯æŒ")
            print("å°è¯•ä½¿ç”¨é»˜è®¤ç¼–ç å™¨åˆ›å»ºAVIæ–‡ä»¶")
            # å›é€€åˆ°AVIæ ¼å¼å’ŒXVIDç¼–ç å™¨[2](@ref)
            self.current_video_path = os.path.join(
                self.video_dir,
                f"segment_{self.segment_count}_{timestamp}.avi"
            )
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            self.out_video = cv2.VideoWriter(
                self.current_video_path,
                fourcc,
                safe_fps,
                (width, height)
            )
            if not self.out_video.isOpened():
                print(f"âŒ ä¸¥é‡é”™è¯¯: æ— æ³•åˆå§‹åŒ–ä»»ä½•è§†é¢‘å†™å…¥å™¨")
                return False

        print(f"\nğŸ”´ å¼€å§‹å½•åˆ¶æ–°è§†é¢‘ç‰‡æ®µ: {self.current_video_path}")
        self.segment_start_time = time.time()
        self.segment_count += 1
        return True

    def predict_from_camera(self, show=True, camera_index=0):
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        cap = cv2.VideoCapture(camera_index)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
        cap.set(cv2.CAP_PROP_FPS, 30)

        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€USBæ‘„åƒå¤´: /dev/video{camera_index}")

        print(f"æˆåŠŸæ‰“å¼€USBæ‘„åƒå¤´: /dev/video{camera_index}")
        print("æŒ‰ 'q' é”®é€€å‡ºå®æ—¶æ£€æµ‹...")

        # è·å–æ‘„åƒå¤´å‚æ•°
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0:
            fps = 30.0  # é»˜è®¤å€¼
        self.actual_fps = fps  # ä¿å­˜å®é™…å¸§ç‡

        print(f"æ‘„åƒå¤´å‚æ•°: {width}x{height} @ {fps:.1f}FPS")

        # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªè§†é¢‘å†™å…¥å™¨
        if not self.init_video_writer(width, height, fps):
            print("âŒ è§†é¢‘å†™å…¥å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
            cap.release()
            return

        # æ€§èƒ½ç›‘æ§
        inference_times = []
        last_report_time = time.time()
        report_interval = 5  # æ€§èƒ½æŠ¥å‘Šé—´éš”(ç§’)
        last_frame_time = time.time()

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    print("æ— æ³•ä»æ‘„åƒå¤´è·å–å¸§")
                    time.sleep(0.1)
                    continue

                # å®Œæ•´å¤„ç†æµç¨‹è®¡æ—¶
                frame_start = time.perf_counter()

                # é¢„å¤„ç†
                blob = self.preprocess(frame)

                # NPUæ¨ç†æ ¸å¿ƒè®¡æ—¶
                infer_start = time.perf_counter()
                outputs = self.model.infer(blob, mode="static")
                infer_elapsed = (time.perf_counter() - infer_start) * 1000

                # è®°å½•å¹¶æ‰“å°æ¨ç†è€—æ—¶
                inference_times.append(infer_elapsed)
                self.total_inference_time += infer_elapsed
                self.frame_count += 1

                # åå¤„ç†ï¼ˆç›®æ ‡æ£€æµ‹å’Œæ ‡è®°ï¼‰
                result_frame = self.postprocess(outputs, frame)

                # å¸§å¤„ç†æ€»è€—æ—¶
                frame_elapsed = (time.perf_counter() - frame_start) * 1000

                # è®¡ç®—å®é™…å¸§ç‡
                current_time = time.time()
                actual_fps = 1.0 / (current_time - last_frame_time)
                last_frame_time = current_time

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°çš„è§†é¢‘ç‰‡æ®µ
                if current_time - self.segment_start_time >= self.segment_interval:
                    # å…³é—­å½“å‰è§†é¢‘æ–‡ä»¶
                    if self.out_video is not None:
                        self.out_video.release()
                        print(f"ğŸŸ¢ è§†é¢‘ç‰‡æ®µå·²ä¿å­˜: {self.current_video_path}")
                        # æ‰“å°å®é™…ä¿å­˜çš„è§†é¢‘å¸§æ•°
                        segment_frame_count = int(self.segment_interval * self.actual_fps)
                        print(f"ä¿å­˜å¸§æ•°: {segment_frame_count}, å®é™…FPS: {actual_fps:.1f}")

                    # åˆå§‹åŒ–æ–°çš„è§†é¢‘å†™å…¥å™¨
                    self.init_video_writer(width, height, self.actual_fps)

                # å°†æ ‡è®°åçš„å¸§å†™å…¥å½“å‰è§†é¢‘æ–‡ä»¶
                if self.out_video is not None:
                    self.out_video.write(result_frame)

                # æ˜¾ç¤ºå¤„ç†åçš„å¸§
                if show:
                    # åœ¨ç”»é¢ä¸Šæ˜¾ç¤ºå®é™…FPS
                    fps_text = f"FPS: {actual_fps:.1f}"
                    cv2.putText(result_frame, fps_text, (10, 30),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                    cv2.imshow('NPUåŠ é€Ÿ-YOLO', result_frame)

                # å®šæ—¶æ˜¾ç¤ºæ€§èƒ½æŠ¥å‘Š
                if current_time - last_report_time >= report_interval:
                    avg_time = np.mean(inference_times) if inference_times else 0
                    fps_estimate = 1000 / avg_time if avg_time > 0 else 0
                    print(f"\nğŸ“Š å®æ—¶æ€§èƒ½æŠ¥å‘Š (è¿‡å»{report_interval}ç§’):")
                    print(f"- å¤„ç†å¸§æ•°: {len(inference_times)}")
                    print(f"- å¹³å‡æ¨ç†è€—æ—¶: {avg_time:.2f}ms")
                    print(f"- é¢„ä¼°FPS: {fps_estimate:.1f}")
                    print(f"- å®é™…FPS: {actual_fps:.1f}")
                    print("-" * 40)

                    # é‡ç½®è®¡æ•°å™¨å’Œæ—¶é—´æˆ³
                    inference_times = []
                    last_report_time = current_time

                # æ£€æŸ¥é€€å‡ºé”®
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        finally:
            # é‡Šæ”¾èµ„æº
            cap.release()
            if self.out_video is not None and self.out_video.isOpened():
                self.out_video.release()
                print(f"ğŸŸ¢ æœ€åä¸€ä¸ªè§†é¢‘ç‰‡æ®µå·²ä¿å­˜: {self.current_video_path}")
            cv2.destroyAllWindows()

            # æœ€ç»ˆæ€§èƒ½æ€»ç»“æŠ¥å‘Š
            if self.frame_count > 0:
                avg_time = self.total_inference_time / self.frame_count
                fps = 1000 / avg_time if avg_time > 0 else 0
                print("\n" + "=" * 50)
                print("æœ€ç»ˆæ€§èƒ½åˆ†ææŠ¥å‘Š:")
                print(f"- æ€»å¤„ç†å¸§æ•°: {self.frame_count}")
                print(f"- å¹³å‡æ¨ç†è€—æ—¶: {avg_time:.2f}ms")
                print(f"- é¢„ä¼°å¹³å‡FPS: {fps:.1f}")
                print(f"- ä¿å­˜è§†é¢‘ç‰‡æ®µæ•°: {self.segment_count}")
                print(f"- è§†é¢‘ä¿å­˜ä½ç½®: {os.path.abspath(self.video_dir)}")
                print("=" * 50)


if __name__ == '__main__':
    inferencer = NPUInferencer(
        model_path="./runs/train/train6/weights/best.om",
        conf_threshold=0.3
    )
    inferencer.predict_from_camera(
        camera_index=0,
        show=False
    )