import cv2
import numpy as np
import time
import os
from datetime import datetime
from tqdm import tqdm
from ais_bench.infer.interface import InferSession


class NPUInferencer:
    def __init__(self, model_path, conf_threshold=0.3):
        self.model = InferSession(device_id=0, model_path=model_path)
        self.conf_threshold = conf_threshold
        self.input_shape = self.model.get_inputs()[0].shape
        self.output_shape = self.model.get_outputs()[0].shape
        self.total_inference_time = 0
        self.frame_count = 0

        # åˆ›å»ºè§†é¢‘ä¿å­˜ç›®å½•
        self.video_dir = "./mp4"
        os.makedirs(self.video_dir, exist_ok=True)

        # è§†é¢‘åˆ†æ®µå‚æ•°
        self.segment_interval = 20  # æ¯20ç§’ä¿å­˜ä¸€ä¸ªè§†é¢‘ç‰‡æ®µ
        self.segment_start_time = 0  # è§†é¢‘æ—¶é—´è€Œéå®æ—¶æ—¶é—´
        self.segment_count = 0
        self.out_video = None
        self.current_video_path = ""
        self.fps = 0  # è§†é¢‘å¸§ç‡
        self.frame_index = 0  # å½“å‰å¸§åœ¨è§†é¢‘ä¸­çš„ä½ç½®

        # ç±»åˆ«æ˜ å°„è¡¨ï¼ˆæ ¹æ®å®é™…æ¨¡å‹ä¿®æ”¹ï¼‰
        self.class_names = {
            0: "buoy"
            # æ·»åŠ æ›´å¤šç±»åˆ«...
        }

        # æ£€æµ‹ç»“æœè®°å½•
        self.detection_log = []

    def preprocess(self, frame):
        start_time = time.perf_counter()
        img = cv2.resize(frame, (self.input_shape[3], self.input_shape[2]))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img.transpose(2, 0, 1)[np.newaxis]
        img = img.astype("float32") / 255.0
        elapsed = (time.perf_counter() - start_time) * 1000
        print(f"é¢„å¤„ç†è€—æ—¶: {elapsed:.2f}ms")
        return img

    def postprocess(self, outputs, frame):
        start_time = time.perf_counter()
        h, w = frame.shape[:2]

        # å­˜å‚¨æ£€æµ‹åˆ°çš„ç›®æ ‡
        detected_objects = []

        # ç¡®å®šè¾“å‡ºæ ¼å¼
        if outputs[0].shape == (1, 8400, 84):
            detections = outputs[0][0]
        else:
            detections = outputs[0][0].transpose(1, 0)

        if detections.size == 0:
            return frame, detected_objects

        for row in detections:
            if len(row) < 6:
                continue
            try:
                x1, y1, x2, y2, conf, cls_id = row[:6]
                if conf < self.conf_threshold:
                    continue

                # è½¬æ¢ä¸ºå›¾åƒåæ ‡
                x1, y1, x2, y2 = int(x1 * w), int(y1 * h), int(x2 * w), int(y2 * h)

                # è·å–ç±»åˆ«åç§°
                class_id = int(cls_id)
                class_name = self.class_names.get(class_id, f"class_{class_id}")

                # è®¡ç®—ä¸­å¿ƒç‚¹åæ ‡
                center_x = (x1 + x2) // 2
                center_y = (y1 + y2) // 2

                # å­˜å‚¨æ£€æµ‹ä¿¡æ¯
                obj_info = {
                    "class_id": class_id,
                    "class_name": class_name,
                    "confidence": float(conf),
                    "bbox": (x1, y1, x2, y2),
                    "center": (center_x, center_y),
                    "frame_index": self.frame_index,
                    "timestamp": self.frame_index / self.fps
                }
                detected_objects.append(obj_info)

                # ç»˜åˆ¶è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
                label = f"{class_name} {conf:.2f}"
                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1)  # ç»˜åˆ¶ä¸­å¿ƒç‚¹
                cv2.putText(frame, label, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            except (ValueError, IndexError) as e:
                print(f"å¤„ç†æ£€æµ‹ç»“æœæ—¶å‡ºé”™: {e}")
                continue

        elapsed = (time.perf_counter() - start_time) * 1000
        print(f"åå¤„ç†è€—æ—¶: {elapsed:.2f}ms")
        return frame, detected_objects

    def log_detection(self, detected_objects):
        """è®°å½•å¹¶æ‰“å°æ£€æµ‹åˆ°çš„ç›®æ ‡ä¿¡æ¯"""
        if not detected_objects:
            return

        # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡
        class_counts = {}
        for obj in detected_objects:
            class_name = obj["class_name"]
            class_counts[class_name] = class_counts.get(class_name, 0) + 1

            # è®°å½•æ£€æµ‹æ—¥å¿—
            self.detection_log.append(obj)

        # æ‰“å°æ£€æµ‹æ‘˜è¦
        timestamp = datetime.now().strftime("%H:%M:%S")
        time_in_video = self.frame_index / self.fps
        print(f"\nğŸ”” æ£€æµ‹åˆ°ç›®æ ‡ (è§†é¢‘æ—¶é—´: {time_in_video:.1f}s, ç³»ç»Ÿæ—¶é—´: {timestamp})")

        # æ‰“å°æ¯ä¸ªç±»åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
        for class_name, count in class_counts.items():
            print(f"  - {class_name}: {count}ä¸ª")

        # æ‰“å°ç½®ä¿¡åº¦æœ€é«˜çš„ç›®æ ‡
        highest_conf_obj = max(detected_objects, key=lambda x: x["confidence"])
        print(f"  æœ€é«˜ç½®ä¿¡åº¦ç›®æ ‡: {highest_conf_obj['class_name']} "
              f"{highest_conf_obj['confidence']:.2%} "
              f"ä½ç½®: ({highest_conf_obj['center'][0]}, {highest_conf_obj['center'][1]})")

    def init_video_writer(self, width, height, fps):
        """åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨"""
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_video_path = os.path.join(
            self.video_dir,
            f"segment_{self.segment_count}_{timestamp}.mp4"
        )

        # è®¾ç½®è§†é¢‘ç¼–ç å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨H.264ç¼–ç ï¼‰
        fourcc_options = [
            cv2.VideoWriter_fourcc(*'avc1'),  # H.264ç¼–ç 
            cv2.VideoWriter_fourcc(*'mp4v'),  # MPEG-4ç¼–ç 
            cv2.VideoWriter_fourcc(*'X264'),  # å¼€æºH.264å®ç°
            cv2.VideoWriter_fourcc(*'XVID')  # å…¼å®¹æ€§ç¼–ç å™¨
        ]

        # å°è¯•ä¸åŒç¼–ç å™¨
        for fourcc in fourcc_options:
            self.out_video = cv2.VideoWriter(
                self.current_video_path,
                fourcc,
                fps,
                (width, height)
            )
            if self.out_video.isOpened():
                print(f"âœ… æˆåŠŸä½¿ç”¨ç¼–ç å™¨åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨: {self.current_video_path}")
                break
            else:
                print(f"âš ï¸ ç¼–ç å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªé€‰é¡¹")

        # å¦‚æœæ‰€æœ‰ç¼–ç å™¨éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹
        if not self.out_video.isOpened():
            self.current_video_path = os.path.join(
                self.video_dir,
                f"segment_{self.segment_count}_{timestamp}.avi"
            )
            self.out_video = cv2.VideoWriter(
                self.current_video_path,
                cv2.VideoWriter_fourcc(*'XVID'),
                fps,
                (width, height)
            )
            print(f"âš ï¸ ä½¿ç”¨å…¼å®¹æ€§ç¼–ç å™¨: {self.current_video_path}")

        # é‡ç½®åˆ†æ®µè®¡æ—¶
        self.segment_start_time = self.frame_index / self.fps
        self.segment_count += 1

    def predict(self, source, show=True):
        cap = cv2.VideoCapture(source)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æº: {source}")

        # è·å–è§†é¢‘å‚æ•°
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        print(f"è§†é¢‘æ€»å¸§æ•°: {total_frames}, FPS: {self.fps:.1f}, åˆ†è¾¨ç‡: {width}x{height}")

        # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªè§†é¢‘å†™å…¥å™¨
        self.init_video_writer(width, height, self.fps)

        # æ€§èƒ½ç›‘æ§æ•°æ®ç»“æ„
        inference_times = []
        self.frame_index = 0

        for _ in tqdm(range(total_frames), desc="NPUæ¨ç†è¿›åº¦"):
            ret, frame = cap.read()
            if not ret:
                break

            # æ›´æ–°å¸§ç´¢å¼•
            self.frame_index += 1

            # å®Œæ•´å¤„ç†æµç¨‹è®¡æ—¶
            frame_start = time.perf_counter()

            # é¢„å¤„ç†
            blob = self.preprocess(frame)

            # NPUæ¨ç†æ ¸å¿ƒè®¡æ—¶
            infer_start = time.perf_counter()
            outputs = self.model.infer(blob, mode="static")
            infer_elapsed = (time.perf_counter() - infer_start) * 1000

            # è®°å½•å¹¶æ‰“å°æ¨ç†è€—æ—¶
            inference_times.append(infer_elapsed)
            self.total_inference_time += infer_elapsed
            self.frame_count += 1
            print(f"NPUæ¨ç†è€—æ—¶: {infer_elapsed:.2f}ms")

            # åå¤„ç†ï¼ˆç›®æ ‡æ£€æµ‹å’Œæ ‡è®°ï¼‰
            result_frame, detected_objects = self.postprocess(outputs, frame)

            # è®°å½•å¹¶æ‰“å°æ£€æµ‹ç»“æœ
            self.log_detection(detected_objects)

            # å¸§å¤„ç†æ€»è€—æ—¶
            frame_elapsed = (time.perf_counter() - frame_start) * 1000
            print(f"å•å¸§æ€»è€—æ—¶: {frame_elapsed:.2f}ms")

            # è®¡ç®—å½“å‰è§†é¢‘æ—¶é—´ï¼ˆç§’ï¼‰
            current_video_time = self.frame_index / self.fps

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°çš„è§†é¢‘ç‰‡æ®µ
            if current_video_time - self.segment_start_time >= self.segment_interval:
                # å…³é—­å½“å‰è§†é¢‘æ–‡ä»¶
                if self.out_video is not None:
                    self.out_video.release()
                    print(f"ğŸŸ¢ è§†é¢‘ç‰‡æ®µå·²ä¿å­˜: {self.current_video_path}")

                # åˆå§‹åŒ–æ–°çš„è§†é¢‘å†™å…¥å™¨
                self.init_video_writer(width, height, self.fps)

            # å°†æ ‡è®°åçš„å¸§å†™å…¥å½“å‰è§†é¢‘æ–‡ä»¶
            if self.out_video is not None and self.out_video.isOpened():
                self.out_video.write(result_frame)

            if show:
                # åœ¨ç”»é¢ä¸Šæ˜¾ç¤ºç‰‡æ®µä¿¡æ¯
                segment_info = f"Segment: {self.segment_count} | Time: {current_video_time:.1f}s"
                cv2.putText(result_frame, segment_info, (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                # æ˜¾ç¤ºæ£€æµ‹è®¡æ•°
                if detected_objects:
                    detection_count = len(detected_objects)
                    detection_info = f"Detections: {detection_count}"
                    cv2.putText(result_frame, detection_info, (10, 60),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

                cv2.imshow('NPUåŠ é€Ÿ-YOLO', result_frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        # é‡Šæ”¾èµ„æº
        cap.release()
        if self.out_video is not None and self.out_video.isOpened():
            self.out_video.release()
            print(f"ğŸŸ¢ æœ€åä¸€ä¸ªè§†é¢‘ç‰‡æ®µå·²ä¿å­˜: {self.current_video_path}")
        cv2.destroyAllWindows()

        # æ€§èƒ½æ€»ç»“æŠ¥å‘Š
        if inference_times:
            avg_time = np.mean(inference_times)
            min_time = np.min(inference_times)
            max_time = np.max(inference_times)
            print("\n" + "=" * 50)
            print(f"æ€§èƒ½åˆ†ææŠ¥å‘Š:")
            print(f"- æ€»å¤„ç†å¸§æ•°: {self.frame_count}")
            print(f"- å¹³å‡æ¨ç†è€—æ—¶: {avg_time:.2f}ms")
            print(f"- æœ€å¿«æ¨ç†è€—æ—¶: {min_time:.2f}ms")
            print(f"- æœ€æ…¢æ¨ç†è€—æ—¶: {max_time:.2f}ms")
            print(f"- é¢„ä¼°FPS: {1000 / avg_time:.1f} (ä»…æ¨ç†)")
            print(f"- ä¿å­˜è§†é¢‘ç‰‡æ®µæ•°: {self.segment_count}")
            print(f"- è§†é¢‘ä¿å­˜ä½ç½®: {os.path.abspath(self.video_dir)}")

            # æ£€æµ‹ç»“æœæ±‡æ€»
            if self.detection_log:
                print("\næ£€æµ‹ç»“æœæ±‡æ€»:")
                class_counts = {}
                for obj in self.detection_log:
                    class_name = obj["class_name"]
                    class_counts[class_name] = class_counts.get(class_name, 0) + 1

                for class_name, count in class_counts.items():
                    print(f"  - {class_name}: {count}æ¬¡")

                # æ‰“å°é¦–æ¬¡å’Œæœ€åä¸€æ¬¡æ£€æµ‹æ—¶é—´
                first_detection = min(self.detection_log, key=lambda x: x["timestamp"])
                last_detection = max(self.detection_log, key=lambda x: x["timestamp"])
                print(f"\né¦–æ¬¡æ£€æµ‹: {first_detection['class_name']} "
                      f"åœ¨ {first_detection['timestamp']:.1f}s")
                print(f"æœ€åä¸€æ¬¡æ£€æµ‹: {last_detection['class_name']} "
                      f"åœ¨ {last_detection['timestamp']:.1f}s")

            print("=" * 50)


if __name__ == '__main__':
    # å®šä¹‰ç±»åˆ«æ˜ å°„ï¼ˆæ ¹æ®å®é™…æ¨¡å‹ä¿®æ”¹ï¼‰
    class_names = {
        0: "buoy"
    }

    inferencer = NPUInferencer(
        model_path="./runs/train/train6/weights/best.om",
        conf_threshold=0.3,
    )

    # è®¾ç½®è‡ªå®šä¹‰ç±»åˆ«æ˜ å°„
    inferencer.class_names = class_names

    inferencer.predict(
        source=r'./datasets/test/30386095338-1-192.mp4',
        show=False
    )
